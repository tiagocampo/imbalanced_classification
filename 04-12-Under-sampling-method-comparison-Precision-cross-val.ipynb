{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a classifier with under-sampling and CV\n",
    "\n",
    "When we train a classifier, we want it to predict an outcome in a real life dataset. The real datasets, most likely, also have a class imbalance. Thus, it is important to evaluate the performance of the classifier, on a data set with the same distribution of classes, as the one we expect in real life.\n",
    "\n",
    "This means, that the under-sampling methods should be performed on the dataset that we are going to use to train the classifier. But, **the performance of the model should be determined on a portion of the data, that was not re-sampled**.\n",
    "\n",
    "In this notebook, we will use the imbalanced-learn pipeline, to set up various under-sampling solutions, in a way that we train the model on re-sampled data, but we evaluate performance on non resampled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from imblearn.datasets import fetch_datasets\n",
    "\n",
    "# to correctly set up the cross-validation\n",
    "from imblearn.pipeline import make_pipeline\n",
    "\n",
    "from imblearn.under_sampling import (\n",
    "    RandomUnderSampler,\n",
    "    CondensedNearestNeighbour,\n",
    "    TomekLinks,\n",
    "    OneSidedSelection,\n",
    "    EditedNearestNeighbours,\n",
    "    RepeatedEditedNearestNeighbours,\n",
    "    AllKNN,\n",
    "    NeighbourhoodCleaningRule,\n",
    "    NearMiss,\n",
    "    InstanceHardnessThreshold\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "undersampler_dict = {\n",
    "\n",
    "    'random': RandomUnderSampler(\n",
    "        sampling_strategy='auto',\n",
    "        random_state=0,\n",
    "        replacement=False),\n",
    "\n",
    "    'cnn': CondensedNearestNeighbour(\n",
    "        sampling_strategy='auto',\n",
    "        random_state=0,\n",
    "        n_neighbors=1,\n",
    "        n_jobs=4),\n",
    "\n",
    "    'tomek': TomekLinks(\n",
    "        sampling_strategy='auto',\n",
    "        n_jobs=4),\n",
    "\n",
    "    'oss': OneSidedSelection(\n",
    "        sampling_strategy='auto',\n",
    "        random_state=0,\n",
    "        n_neighbors=1,\n",
    "        n_jobs=4),\n",
    "\n",
    "    'enn': EditedNearestNeighbours(\n",
    "        sampling_strategy='auto',\n",
    "        n_neighbors=3,\n",
    "        kind_sel='all',\n",
    "        n_jobs=4),\n",
    "\n",
    "    'renn': RepeatedEditedNearestNeighbours(\n",
    "        sampling_strategy='auto',\n",
    "        n_neighbors=3,\n",
    "        kind_sel='all',\n",
    "        n_jobs=4,\n",
    "        max_iter=100),\n",
    "\n",
    "    'allknn': AllKNN(\n",
    "        sampling_strategy='auto',\n",
    "        n_neighbors=5,\n",
    "        kind_sel='all',\n",
    "        n_jobs=4),\n",
    "\n",
    "    'ncr': NeighbourhoodCleaningRule(\n",
    "        sampling_strategy='auto',\n",
    "        n_neighbors=3,\n",
    "        kind_sel='all',\n",
    "        n_jobs=4,\n",
    "        threshold_cleaning=0.5),\n",
    "\n",
    "    'nm1': NearMiss(\n",
    "        sampling_strategy='auto',\n",
    "        version=1,\n",
    "        n_neighbors=3,\n",
    "        n_jobs=4),\n",
    "\n",
    "    'nm2': NearMiss(\n",
    "        sampling_strategy='auto',\n",
    "        version=2,\n",
    "        n_neighbors=3,\n",
    "        n_jobs=4),\n",
    "\n",
    "    # we set up the instance hardness threshold\n",
    "    # with the same classifier that we intend to use in our data\n",
    "    'iht': InstanceHardnessThreshold(\n",
    "        estimator=RandomForestClassifier(\n",
    "            n_estimators=100, random_state=39, max_depth=3, n_jobs=4,\n",
    "        ),\n",
    "        sampling_strategy='auto',\n",
    "        random_state=0,\n",
    "        n_jobs=4,\n",
    "        cv=3)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these datasets are baked into imbalanced-learn\n",
    "\n",
    "datasets_ls = [\n",
    "    'car_eval_34',\n",
    "    'ecoli',\n",
    "    'thyroid_sick',\n",
    "    'arrhythmia',\n",
    "    'ozone_level'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car_eval_34\n",
      "Counter({-1: 1594, 1: 134})\n",
      "\n",
      "ecoli\n",
      "Counter({-1: 301, 1: 35})\n",
      "\n",
      "thyroid_sick\n",
      "Counter({-1: 3541, 1: 231})\n",
      "\n",
      "arrhythmia\n",
      "Counter({-1: 427, 1: 25})\n",
      "\n",
      "ozone_level\n",
      "Counter({-1: 2463, 1: 73})\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print class imbalance of each dataset\n",
    "\n",
    "for dataset in datasets_ls:\n",
    "    data = fetch_datasets()[dataset]\n",
    "    print(dataset)\n",
    "    print(Counter(data.target))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train random forests and evaluate the performance\n",
    "\n",
    "# ***with cross-validation***\n",
    "\n",
    "def run_model(X_train, y_train, undersampler=None):\n",
    "    \n",
    "    # set up the classifier\n",
    "    rf = RandomForestClassifier(\n",
    "            n_estimators=100, random_state=39, max_depth=3, n_jobs=4\n",
    "        )\n",
    "    \n",
    "    # set up a scaler \n",
    "    # (as many undersampling techniques use KNN\n",
    "    # we put the variables in the same scale)\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    # without undersampling:\n",
    "    if not undersampler:\n",
    "\n",
    "        model = rf\n",
    "    \n",
    "    # set up a pipeline with undersampling:\n",
    "    else:\n",
    "        \n",
    "        # important to scale before the under-sampler\n",
    "        # as the many of methods require the variables in \n",
    "        # a similar scale\n",
    "        model = make_pipeline(\n",
    "            scaler,\n",
    "            undersampler,\n",
    "            rf,\n",
    "        )\n",
    "        \n",
    "        \n",
    "    # When we make a pipeline and then run the training of the model\n",
    "    # with cross-validation, the procedure works as follows:\n",
    "    \n",
    "    # 1) take 2 of the 3 fold as train set\n",
    "    # 2) resample the 2 fold (aka, the train set)\n",
    "    # 3) train the model on the resampled data from point 2\n",
    "    # 4) evaluate performance on the 3rd fold, that was not resampled\n",
    "    \n",
    "    # this way, we make sure that we are not evaluating the performance\n",
    "    # of our classifier on the under-sampled data\n",
    "    \n",
    "    cv_results = cross_validate(\n",
    "        model, # the random forest or the pipeline\n",
    "        X_train, # the data that will be used in the cross-validation\n",
    "        y_train, # the target\n",
    "        scoring=\"average_precision\", # the metric that we want to evaluate\n",
    "        cv=3, # the cross-validation fold\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        'Random Forests average precision: {0} +/- {1}'.format(\n",
    "        cv_results['test_score'].mean(), cv_results['test_score'].std()\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return cv_results['test_score'].mean(), cv_results['test_score'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# now we train several models, with the different under-samplers and\n",
    "# with cross-validation for each dataset\n",
    "\n",
    "# to save the results\n",
    "pr_mean_dict = {}\n",
    "pr_std_dict = {}\n",
    "\n",
    "\n",
    "for dataset in datasets_ls:\n",
    "    \n",
    "    # initiate a dictionary per dataset\n",
    "    pr_mean_dict[dataset] = {}\n",
    "    pr_std_dict[dataset] = {}\n",
    "    \n",
    "    print(dataset)\n",
    "    \n",
    "    # load dataset\n",
    "    data = fetch_datasets()[dataset]\n",
    "    \n",
    "    # separate dataset into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        data.data,  \n",
    "        data.target, \n",
    "        test_size=0.3,\n",
    "        random_state=0,\n",
    "    )\n",
    "   \n",
    "    \n",
    "    # train model on data without re-sampling\n",
    "    # with cross-validation\n",
    "    aps_mean, aps_std = run_model(X_train, y_train)\n",
    "    \n",
    "    # store results\n",
    "    pr_mean_dict[dataset]['full_data'] = aps_mean\n",
    "    pr_std_dict[dataset]['full_data'] = aps_std\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    for undersampler in undersampler_dict.keys():\n",
    "        \n",
    "        print(undersampler)\n",
    "               \n",
    "        # resample, train and evaluate performance\n",
    "        # with cross-validation\n",
    "        aps_mean, aps_std = run_model(X_train, y_train, undersampler_dict[undersampler])\n",
    "        \n",
    "        #store results\n",
    "        pr_mean_dict[dataset][undersampler] = aps_mean\n",
    "        pr_std_dict[dataset][undersampler] = aps_std\n",
    "        print()\n",
    "        \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we plot the performance of the model in the \n",
    "# left out fold, that was not resampled, from the X_train\n",
    "\n",
    "for dataset in datasets_ls:\n",
    "    \n",
    "    pr_mean_s = pd.Series(pr_mean_dict[dataset])\n",
    "    pr_std_s = pd.Series(pr_std_dict[dataset])\n",
    "    \n",
    "    pr_mean_s.plot.bar(yerr=[pr_std_s, pr_std_s]\n",
    "        )\n",
    "    plt.title(dataset)\n",
    "    plt.ylabel('Average Precision')\n",
    "    plt.axhline(pr_mean_dict[dataset]['full_data'], color='r')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Different under-sampling techniques work best for different datasets.**\n",
    "- ENN, RENN and AllKNN tend to produce similar results, so we may as well just choose one of the 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
